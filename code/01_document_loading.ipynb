{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Document Loading\n",
                "\n",
                "Learn the fundamentals of data loading and discover over 80 unique loaders LangChain provides to access diverse data sources, including audio and video.\n",
                "\n",
                "# Setup\n",
                "\n",
                "## Python"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from langchain.document_loaders import NotionDirectoryLoader\n",
                "from langchain.document_loaders import WebBaseLoader\n",
                "import pandas as pd\n",
                "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
                "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
                "from langchain.document_loaders.generic import GenericLoader\n",
                "from langchain.document_loaders import PyPDFLoader\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "import os\n",
                "import openai\n",
                "# import sys\n",
                "# sys.path.append('../..')\n",
                "\n",
                "_ = load_dotenv(find_dotenv())  # read local .env file\n",
                "\n",
                "openai.api_key = os.environ['OPENAI_API_KEY']"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retrieval Augmented Generation (RAG)\n",
                " \n",
                "## Basics\n",
                "\n",
                "- In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n",
                "\n",
                "- This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). \n",
                "\n",
                "![](/images/rag.png)\n",
                "\n",
                "\n",
                "# PDF \n",
                "\n",
                "## Example\n",
                "\n",
                "- Let's load a PDF [transcript](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf) from one of Andrew Ng's courses\n",
                "\n",
                "- These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly.\n",
                "\n",
                "## Load PDF"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "loader = PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
                "pages = loader.load()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Each page is a `Document`.\n",
                "\n",
                "- A `Document` contains text (`page_content`) and `metadata`.\n",
                "\n",
                "## Inspect data "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "len(pages)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- 22\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "page = pages[0]"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "page.metadata"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'source': '../docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n",
                "\n",
                "\n",
                "## Inspect content {.smaller}\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print(page.page_content[0:500])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- MachineLearning-Lecture01  \n",
                "Instructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \n",
                "learning class. So what I wanna do today is ju st spend a little time going over the logistics \n",
                "of the class, and then we'll start to  talk a bit about machine learning.  \n",
                "By way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \n",
                "I personally work in machine learning, and I' ve worked on it for about 15 years now, and \n",
                "I actually think that machine learning i\n",
                "\n",
                "# YouTube\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- You need [FFmpeg](https://ffmpeg.org/) \n",
                "\n",
                "- Mac: [install with Homebrew](https://formulae.brew.sh/formula/ffmpeg)\n",
                "\n",
                "## Example\n",
                "\n",
                "Let's load the \"Code Report\" about Vector databases from Fireship\n",
                "\n",
                "\n",
                " <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/klTvEwg3oJ4?si=VPuxdiw9QaWfIqbD\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe> \n",
                "\n",
                "\n",
                "## Load YouTube video\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# link to video\n",
                "url = \"https://www.youtube.com/watch?v=klTvEwg3oJ4\"\n",
                "\n",
                "# path to directory\n",
                "save_dir = \"../docs/youtube/\"\n",
                "\n",
                "# load video\n",
                "loader = GenericLoader(\n",
                "    YoutubeAudioLoader([url], save_dir),\n",
                "    OpenAIWhisperParser()\n",
                ")\n",
                "\n",
                "docs = loader.load()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inspect data"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "docs[0].page_content[0:500]"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- \"It is April 7th, 2023, and you're watching The Code Report. One month ago, Vector Database Weaviate landed $16 million in Series A funding. Last week, PineconeDB just got a check for $28 million at a $700 million valuation. And yesterday, Chroma, an open source project with only 1.2 GitHub stars, raised $18 million for its Embeddings database. And I just launched my own Vector database this morning. We're currently pre-revenue, pre-vision, and pre-code, and valued at $420 million. Leave your cre\"\n",
                "\n",
                "\n",
                "## Save data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df = pd.DataFrame(docs, columns=['Text', 'Metadata'])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df.to_csv('../docs/youtube/codereport.csv')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# URLs\n",
                "\n",
                "## Example\n",
                "\n",
                "- Let's load a page from \"Introduction to Modern Statistics\" by Mine Ã‡etinkaya-Rundel and Johanna Hardin: <https://openintro-ims.netlify.app/data-design>\n",
                "\n",
                "- The raw file is provided in GutHub under this URL: <https://raw.githubusercontent.com/OpenIntroStat/ims/main/02-data-design.qmd>\n",
                "\n",
                "## Load URL"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "loader = WebBaseLoader(\n",
                "    \"https://raw.githubusercontent.com/OpenIntroStat/ims/main/02-data-design.qmd\")\n",
                "\n",
                "docs = loader.load()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inspact data"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print(docs[0].page_content[400:800])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- ampling. Knowing how the observational units were selected from a larger entity will allow for generalizations back to the population from which the data were randomly selected.\n",
                "Additionally, by understanding the structure of the study, causal relationships can be separated from those relationships which are only associated.\n",
                "A good question to ask oneself before working with the data at all is, \"H\n",
                "\n",
                "## Save data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df = pd.DataFrame(docs, columns=['Text', 'Metadata'])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df.to_csv('../docs/url/study-design.csv')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notion\n",
                "\n",
                "![](/images/notion.png)\n",
                "\n",
                "## Example\n",
                "\n",
                "- Option 1: Simply use the example data provided in `langchain-intro/docs/Notion_DB` \n",
                "\n",
                "- Option 2: Follow the steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f)\n",
                "  - Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
                "  - Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
                " \n",
                "## Load Notion"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "loader = NotionDirectoryLoader(\"../docs/Notion_DB\")\n",
                "docs = loader.load()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inspect data"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print(docs[0].page_content[0:200])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```markdown\n",
                "# Getting Started\n",
                "\n",
                "ðŸ‘‹ Welcome to Notion!\n",
                "\n",
                "Here are the basics:\n",
                "\n",
                "- [ ]  Click anywhere and just start typing\n",
                "- [ ]  Hit `/` to see all the types of content you can add - headers, videos, sub pages, etc.\n",
                "\n",
                "```\n",
                "\n",
                "## Inspect data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "docs[0].metadata"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}