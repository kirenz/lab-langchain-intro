---
title: "Slides"
number-sections: true
---

*The following tutorials are mainly based on the excellent course ["LangChain: Chat with Your DataI"](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/) provided by Harrison Chase from LangChain and Andrew Ng from DeepLearning.AI and cover the following topics:*


![](/images/rag.png)

::: {.callout-note appearance="simple"}
Take a look at the [slides tutorial](https://kirenz.github.io/lab-toolkit/slides/slides.html#/title-slide) to learn how to use all slide options. 

To download Jupyter Notebooks: Go to the Colab menu and choose "File" > "Download" > "Download .ipynb"
:::

## What is Retrieval-Augmented Generation (RAG)?


<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/T-D1OfcDW1M?si=E491V0ebqfGxV5UZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Large language models usually give great answers, but because they're limited to the training data used to create the model, over time they can become incomplete--or worse, generate answers that are just plain wrong. One way of improving the LLM results is called "retrieval-augmented generation" or RAG. 

In this video, IBM Senior Research Scientist Marina Danilevsky explains the LLM/RAG framework and how this combination delivers two big advantages, namely: the model gets the most up-to-date and trustworthy facts, and you can see where the model got its info, lending more credibility to what it generates.



## Document Loading

Learn the fundamentals of data loading and discover over 80 unique loaders LangChain provides to access diverse data sources, including audio and video:

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/01_document_loading.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/01_document_loading.ipynb)
:::


## Document Splitting

In the context of building LLM-related applications, splitting is the process of breaking down large pieces of text into smaller segments:

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/02_document_splitting.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/02_document_splitting.ipynb)
:::

## Vector stores and embeddings

Learn the concept of embeddings and explore vector store integrations within LangChain.

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/03_vectorstores_and_embeddings.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/03_vectorstores_and_embeddings.ipynb)
:::

## Retrieval

Learn advanced techniques for accessing and indexing data in the vector store, enabling you to retrieve the most relevant information beyond semantic queries:

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/04_retrieval.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/04_retrieval.ipynb)
:::


## Question Answering

Build a one-pass question-answering solution.

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/05_question_answering.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/05_question_answering.ipynb)
:::

## Chat System

Learn how to track and select pertinent information from conversations and data sources, as you build your own chatbot using LangChain.

::: {.callout-tip appearance="simple" icon=false}
- [ğŸ–¥ï¸ Presentation](/slides/06_chat.qmd)
- [ğŸ’» Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-langchain-rag/blob/main/code/06_chat.ipynb)
:::